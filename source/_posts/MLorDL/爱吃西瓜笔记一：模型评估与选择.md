---
title: (爱吃西瓜)笔记一：模型评估与选择
tags: []
id: '117'
categories:
  - - MLorDL
comments: true
date: 2020-02-28 22:12:23
---

## 经验误差与过拟合

假设有m个样本，a个分类出现了错误，那么错误率$E$为

$$
E=\frac{a}{m}
$$

$$
精度=1 - 错误率
$$

### 误差

![不不不 P1](https://img.wush.cc/16311014460383.png?imageView2/0/format/webp/q/80)

预测输出与样本的真实输出之间的差异称为**“误差”**

学习器在训练集上的误差称为“训练误差”或者“经验误差”

在新样本上的误差称为“泛化误差”

### 过拟合与欠拟合

我们总是希望能够将误差最小化，最终获得准确度高，泛化能力较好的模型。这两者在一定程度是是相互冲突的。模型在测试集的准确度“太好”的时候，泛化能力会下降，在新的样本中表现不佳，称为**“过拟合” **；然而，模型训练的不够好，那么同样无法准确对样本进行预测，这个时候的泛化能力几乎毫无意义，称为**“欠拟合”**

![](https://img.wush.cc/16311014461671.png?imageView2/0/format/webp/q/80)

![截屏2020-02-28下午4.16.53](https://img.wush.cc/16311014461191.png?imageView2/0/format/webp/q/80)

在现实任务中，我们通常有多种学习算法可供选择，甚至不同的参数配置，都会产生不同的模型，那么，我们需要选择合适的学习算法和参数配置，这就是机器学习中的**“模式选择（model selection）”**问题。

## 评估方法

通过**“测试集(Testing Set)”**来测试学习器对新样本的判断能力，然后把测试集的“测试误差”近似看做泛化误差。

测试样本要满足：

* 测试样本从样本真实分布中独立同分布采样而得
* 测试集与训练集尽量互斥

### 留出法

“留出法”（hold-out）直接将数据集$D$划分为两个互斥的集合，其中一个作为训练集$S$，另外一个作为测试集$T$。

以二分类举例，假设D=1000,S=700,那么T=300

经过测试集测试，假设有90个样本出现错误，那么$TestingError = \frac{90}{300} \times 100\% = 30\%$，相应的精度为$70\%$。

在进行划分时，要注意数据集中尽量包含各种类别的数据，还要注意多次划分，重复训练评估。因为我们每次划分训练出来的结果都可能存在差异，所以单次评估一般是不可靠的，需要进行多次评估取平均值。

对于数据集，一般取**$\frac{2}{3} - \frac{4}{5}$**作为训练集，剩余部分作为测试集。

### 交叉验证法

将数据集D划分为k个大小相似的互斥子集，每个子集决斗尽可能保持数据分布的一致性：

![截屏2020-02-28下午4.18.44](https://img.wush.cc/16311014459782.png?imageView2/0/format/webp/q/80)

每次使用k-1个作为训练集，剩下1个作为测试集，进行k次测试，获得k个验证结果，将平均值作为返回结果。所以交叉验证法称为“k折交叉验证”(k-fold cross validation)。如图所示为10折交叉验证过程：

![截屏2020-02-28下午4.20.05](https://img.wush.cc/16311014461143.png?imageView2/0/format/webp/q/80)

为了减少因为样本划分不同而产生的误差，我们在进行k折交叉验证的时候随机使用不同的划分重复p次，最后取p次结果的均值，称为：p次k折交叉验证。比如常见的“10次10折交叉验证”

* 特例：留一法
  
  数据集中的样本只保留一个样本进行测试，这样就不会收到随机样本划分而带来的影响，因此留一法的评估结果一般比较准确
  
  缺点：数据量较大时，计算量过大。

### 自助法

自助法以自助采样为基础，给定包含m个数据集的样本$D$，我们对他进行采样产生数据集$D’$:每次随机从D中挑选一个样本，**拷贝**(样本保留在D中，以便下次采样依然可能被采到)放入$D'$；重复m次，获得了包含m个样本的数据集$D'$。可以得到：样本在m次采样中始终不被采到的概率为$(1-\frac{1}{m})^m$,取极限获得：

![截屏2020-02-28下午4.39.03](https://img.wush.cc/16311014461095.png?imageView2/0/format/webp/q/80)

我们可将$D'$作为训练集，$D$/$D'$作为测试集。实际评估的模型与期望评估的模型都是用m个训练样本，但是我们仍然有大约1/3的数据没有在训练集中出现，可以用于测试，这样的测试结果也被称为“包外估计”(out-of-bag estimate).

这种方法适用于数据量较小，难以有效划分的情况下，此外，对集成学习等方法有很大的帮助。

但是注意，因为改变了原始数据集的分布，这会引入估计误差，因此在数据量充足时，留出法和交叉验证法更常用。

## 性能度量

性能度量(performance measure)：衡量模型泛化能力的评价标准

性能度量反应了任务需求，模型的好坏是相对的，不进取决于算法和数据，还取决于任务需求。

在预测任务中，给定例集$D = {(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$，其中$y_i$是示例$x_i$的真实标记，评估学习器$f$的性能，要把预测结果$f(x)$与真实标记$y$进行比较。

回归任务最常用的性能度量是“均方差”(mean squared error)

$$
E(f;D)=\frac{1}{m} \sum_{i=1}^{m}(f(x_i)-y_i)^2
$$



更一般的，对于数据分布和概率密度函数$p(.)$,均方差可描述为：

![截屏2020-02-28下午4.56.10](https://img.wush.cc/16311014457181.png?imageView2/0/format/webp/q/80)

### 错误率与精度

错误率：分类错误占的比例

![截屏2020-02-28下午5.04.43](https://img.wush.cc/16311014458858.png?imageView2/0/format/webp/q/80)

![截屏2020-02-28下午5.04.00](https://img.wush.cc/16311014458901.png?imageView2/0/format/webp/q/80)

精度：分类正确占的比例

![截屏2020-02-28下午5.05.00](https://img.wush.cc/16311014458798.png?imageView2/0/format/webp/q/80)

![截屏2020-02-28下午5.05.12](https://img.wush.cc/16311014458950.png?imageView2/0/format/webp/q/80)

这两个是最常用的性能度量.

### 查准率、查全率与$F1$

查准率(precision, P) $\to$ 准确度

查全率(recall, R) $\to$ 召回率

错误率和精度虽然常用，但是不能满足所有的需求。

这里仅将书中几处介绍查准率(Precision)和查全率(Recall)意义的描述摘录出来。

> (1)瓜农拉来一车西瓜，我们用训练好的模型对这些西瓜进行判别，查准率衡量的是“挑出的西瓜中有多少比例是好瓜”，查全率衡量的是“所有好瓜中有多少比例被挑出来了”。
> 
> (2)信息检索中，查准率衡量的是“检索出的信息中有多少比例是用户感兴趣的”，查全率衡量的是“用户感兴趣的信息中有多少被检索出来了”。
> 
> 商品推荐系统中，为了尽可能少的打扰用户，更希望推荐内容确是用户感兴趣的，此时查准率更重要；在逃犯信息检索系统中，更希望尽可能少漏掉逃犯，此时查全率更加重要。

对于二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分为：

真正例(True Positive, TP)、假正例(Flase Positive, FP)、真反例(True Negative, TN)、假反例(False Negative, FN)

$D_总 = TP+FP+TN+FN$

![截屏2020-02-28下午7.33.59](https://img.wush.cc/16311014459000.png?imageView2/0/format/webp/q/80)

$P = \frac{TP}{TP+FP}$

$R = \frac{TP}{TP+FN}$

查准率是一对矛盾的度量，一般来说，查准率高，查全率就低。但是在一些简单的情况下这两者都可以很高

很多情况下，更具预测结果对样例排序，排在前面的是认为最可能是正例的样本，后面的是最不可能是正例的样本。

按照次顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率，作$P-R$图

![截屏2020-02-28下午7.48.02](https://img.wush.cc/16311014456641.png?imageView2/0/format/webp/q/80)

若一个学习器的"P-R"曲线被另一个学习器的曲线完全“包住”，则可断言，后者的性能优于前者，例如图中AB优于C。但是如果交叉，那就很难进行断言孰优孰劣，例如AB。

平衡点(Break-Even Point, BEP)就是综合考虑P、R的性能度量，他是$P = R$时的取值，例如图中C的$BEP=0.64$，A的$BEP = 0.8$基于平衡点的比较，可以认为A优于B。

**但是**,EBP还是过于简化，更常用的是F1度量：

$$F1 = \frac{2 \times P \times R}{P + R} = \frac{2 \times TP}{D_总 + TP - TN}$$

> ![截屏2020-02-28下午9.02.50](https://img.wush.cc/16311014456656.png?imageView2/0/format/webp/q/80)
> 
> 已知数据集D，其中标记 $ y_i \in {0,1}$ （1 表示正例，0 表示反例）；设$x_i$ 的二值化预测结果为 $h_i \in {0，1}$，则
> 
> ![截屏2020-02-28下午9.05.41](https://img.wush.cc/16311014456669.png?imageView2/0/format/webp/q/80)

调和平均：

$$\frac{1}{F1} = \frac{1}{2}(\frac{1}{P}+\frac{1}{R})$$

F1 是查准率和查全率的合成指标，可以理解为综合考虑了查准率和查全率的性能度量指标，这个指标更加面向类别不平衡问题（参见第 66~67 页第 3.6 节），例如某数据集 99%为正例，剩余 1%为反例，此时分类器只须一直预测正例即可获得 1%的错误率，然后这显然并没有什么意义。

由于一些特殊情况下，对于P与R的重视程度不同,所以给出更一般的式子：

$$
F_\beta = \frac{(1+\beta^2)\times P \times R}{(\beta^2 \times P)+R}
$$



加权调和平均

$$
\frac{1}{F_\beta} = \frac{1}{1+\beta^2}(\frac{1}{P}+\frac{\beta^2}{R})
$$



$\beta \to$R对P相对重要性：

$\beta=1 \to$同样重要，等于F1

$\beta>1 \to$R更重要

$\beta<1 \to$P更重要

很多时候我们有多个二分类混合矩阵，比如：

![截屏2020-02-28下午8.36.40](https://img.wush.cc/16311014456683.png?imageView2/0/format/webp/q/80)

我们希望在n个二分类混淆矩阵上综合考察P和R:

* 方法一：
  
  分别计算各混淆矩阵的P和R,记作：
  
  $$
  (P_1,R_1),(P_2,R_2),...,(P_n,R_n)
  $$
  
  
  
  求平均值：
  
  宏查准率：$macro-P=\frac{1}{n}\sum_{i=1}^np_i$
  
  宏查全率：$macro-R=\frac{1}{n}\sum_{i=1}^nR_i$
  
  宏F1:$macro-F1 = \frac{2\times macro-P \times macro-R}{macro-P+macro-R}$

* 方法二：
  
  将混淆矩阵各对应元素平均，得到：
  
  $$
  \overline{TP},\overline{FP},\overline{TN},\overline{FN}
  $$
  
  
  
  根据结果计算：
  
  微查准率：$macro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$
  
  微查全率：$macro-R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$
  
  微F1:$macro-F1 = \frac{2\times macro-P \times macro-R}{macro-P+macro-R}$

### ROC与AUC

受试者工作特征(Receiver Operating Characteristic, ROC)

ROC曲线下面积(Area Under ROC Curve)

很多学习器是微测试样本产生一个实值或者概率预测，然后将这个预测值与一个分类阈值(threshold)进行比较，若大于阈值则为正例，小于阈值则为反例。例如logistic回归中使用sigmod函数将输出限制在0-1，大于0.5为True，反之则为False。

根据实值或者概率预测结果，我们将测试样本按照可能性排序，分类过程相当于在这个序列中选取一个间断点来将样本分为不同的两个部分，前一部分为“正例”，后一部分为“反例”。

不同任务选取不同点，若重视P,则靠前，若重视R,则靠后。

因此排序的质量体现了“一般情况下”泛化性能的好坏，ROC曲线则是从这个角度出发研究学习器的泛化性能。

根据学习器对样例的排序，按此顺序逐个把样本作为正例预测，每次计算两个值绘制ROC曲线：

* 纵轴——“真正例率”(True Positive Rate, TPR)

* 横轴——“假正例率”(False positive Rate, FPR)

两者分别定义为：

![截屏2020-02-28下午9.15.10](https://img.wush.cc/16311014456698.png?imageView2/0/format/webp/q/80)

![截屏2020-02-28下午9.15.33](https://img.wush.cc/16311014457207.png?imageView2/0/format/webp/q/80)

对曲线的解释：

先解释两种特殊情形，即“**对角线对应于‘随机猜测’模型，而点(0,1)则对应于将所有正例排在所有反例之前的‘理想模型’**”。

看一下 ROC 绘图过程：

给定 $m^+$个正例和$m^-$个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为 0（无样例被预测为正例，因此真正例 TP 和假正例 FP 均为 0，根据公式可知真正例率 TPR 和假正例率 FPR 均为 0，在坐标(0,0)处标记一个点，然后将分类阙值依次设为每个样例的预测值，依次将每个样例划分为正例，设前一个标记点坐标为(x,y)：

若当前为真正例,坐标为$(x,y+\frac{1}{m^+})$

若当前为假正例,坐标为$(x+\frac{1}{m^-},y)$

学习器比较时，若一个包住另一个，则可说前者优于后者，若有交叉，则比较AUC大小。

$$
AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)(y_i+y_{i+1})
$$



为了更好理解，我们将式子变形为：

$$
AUG = \sum_{i=1}^{m-1}(x_{i+1}-x_i)\frac{(y_i+y_{i+1})}{2}
$$



这样可以看出$(x_{i+1}-x_i)$是矩阵的底,$\frac{(y_i+y_{i+1})}{2}$ 是矩阵的高.

排序“损失”(loss)定义为：

$$
l_{rank} = \frac{1}{m^+m^-}\sum_{x^+ \in D^+}\sum_{x^- \in D^-}(Ⅱ(f(x^+)<f(x^-))+\frac{1}{2}Ⅱ(f(x^+)=f(x^-)))
$$



且:

$$
AUC = 1-l_{rank}
$$



### 代价敏感错误率与代价曲线

现实任务中，不同的错误会产生不同的后果。为了权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”（unequal cost）

以二分类问题为例，我们可以根据任务的领域知识设定为一个**"代价矩阵"(cost matrix)**:

![截屏2020-02-29下午7.21.33](https://img.wush.cc/16311014456715.png?imageView2/0/format/webp/q/80)

$cost_{ij}$表示将第$i$类样本预测为第$j$类样品的代价，一般来说$cost_{ii}=0$，若将第0类判别为第1类所造成的损失更大，则$cost_{01}>cost_{10}$。

**损失程度相差越大，$cost_{01}, cost_{10}$值的差别越大**

前面介绍的性能度量大都隐式假设了均等代价，并没有考虑不同错误造成的不同后果。在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化**“总体代价”(total cost)**。

我们将上表中第0类作为正类，第1类作为反类，另$D^+$与$D^-$分别表示D的正例子集和反例子集，则**“代价敏感错误率”(cost-sensitive)**为:

$$
E(f;D;cost)=\frac{1}{m}(\sum_{x_i \in D^+}Ⅱ(f(x_i) \not =y_i)\times cost_{01}+\sum_{x_i \in D^-}Ⅱ(f(x_i) \not =y_i)\times cost_{10})
$$



其中$m^+$和$m^-$表示正例子集和反例子集的样本个数。本式已经假定了样例集D中样本的类别信息已知，其中：

* $\frac{1}{m^+}\sum_{x_i\in D^+}Ⅱ(f(x_i \not= y_i))$表示正例子集预测错误样本所占比例，即**假反例率FNR**

* $\frac{1}{m^-}\sum_{x_i\in D^-}Ⅱ(f(x_i \not= y_i))$表示反例子集预测错误样本所占比例，即**假正例率FPR**

* 因此，对于$E(f;D；cost)$ ，若用$p$表示样例为正例的概率，则样例为反例的概率为($1-p$) ，上式可进一步写为：
  
  ![截屏2020-02-29下午7.41.46](https://img.wush.cc/16311014457233.png?imageView2/0/format/webp/q/80)
  
  这实际上就是**期望代价**的表达式.
  
  在非均等代价下，ROC曲线不能直接反应学习器的期望总体代价，而**“代价曲线”(cost surve)**可以实现。代价曲线的横轴取值为$[0,1]$的正例概率代价：
  
  $$
  P(+)cost = \frac{p \times cost_{01}}{p \times cost_{01}+(1-p)\times cost_{10}}
  $$
  
  纵轴是取值为[0,1]的归一化代价
  
  $$
  cost_{normal} = \frac{FNR\times p \times cost_{01}+FPR\times (1-p) \times cost_{10}}{p \times cost_{01}+(1-p)\times cost_{10}}
  $$
  
  

FNR = 1-FPR

![截屏2020-02-29下午7.50.02](https://img.wush.cc/16311014456733.png?imageView2/0/format/webp/q/80)

## 比较检验

为什么要做比较检验？

机器学习中性能比较非常复杂，要涉及很多因素：

* 我们希望比较的是泛化性能，但是通过实验评估方法我们获得的测试集上的性能，两者对比结果未必相同。
* 测试集上的性能与测试集本身有很大的关系，测试集不同，最终结果一般也不同。
* 很多机器学习算法本身有一定的随机性，即使使用相同参数设置同一个训练集多次运行，结果也会不同。

简单来说，就是通过 评估方法，得到每个算法/模型在各个性能度量上的表现，但这些值实际上是一个随机变量，因此并不能简单用比较大小来说明多个算法的优劣。

如果不做算法研究，不需要写论文对比多种算法的性能，这部分可以暂时跳过(显然我是跳不过了)；另外，有关检测变量的公式，并不需要清楚是怎么来的（这是统计学家要做的事情），我们只需知道如何由测试结果计算出这些检测变量，并根据检测变量服从的分布类型得到临界值，然后将检测变量与临界值对比即可。在这一部分中，将主要进行书中公式的注解，相当一部分内容来自**《机器学习（西瓜书）注解》**。

**，所以统计检验假设(hypothesis test)**为我们进行学习器性能比较提供了重要依据。

在本部分中，默认使用错误率$\epsilon$作为性能度量。

### 假设检验

现实任务中，我们并不知道学习器的泛化错误率，只能获知其测试错误率$\hat{\epsilon}$，泛化错误率与测试错误率未必相同，但是二者接近的可能性比较大，因此可以通过测试错误率估推出泛化错误率的分布。

**式** **(2.26)** **的解释**

这个公式很容易理解，就是概率论中典型的有放回抽样问题。为了便于说明问题，这里将符号表达改一下：泛化错误率为$\varepsilon=\epsilon$的学习器，被测得测试错误率为$\hat{\varepsilon}=\hat{\epsilon}$的概率为：

![截屏2020-02-29下午8.38.47](https://img.wush.cc/16311014457260.png?imageView2/0/format/webp/q/80)

即$\hat{\varepsilon}，\varepsilon$表示变量，$\hat{\epsilon}，\hat{\epsilon}$表示变量值，其中：

![截屏2020-02-29下午8.40.17](https://img.wush.cc/16311014457286.png?imageView2/0/format/webp/q/80)

即中学时学的组合数，表示从包含m个元素的集合中不重复地抽取$\hat{\varepsilon} \times m$个元素的可能取法，中学课本中记为$C_m^{\hat{\varepsilon}\times m}$。

以上公式中，若已知$\varepsilon = \epsilon$，求$\hat{\varepsilon}$为任意值时的条件概率，即后验概率$P(\hat{\varepsilon}\varepsilon=\epsilon)$；反之，若已知$\hat\varepsilon =\hat \epsilon$，求其由$\varepsilon$为任意值时导致$\hat\varepsilon =\hat \epsilon$的概率，即似然概率 。后验概率和似然概率区别在于：对于 P(果因)，若已知“因”的取值则 P(果因)为后验概率，即在知道“因”的取值后“果”发生的概率；反之若已知“果”的取值则 P(果因)为似然概率，即当前的“果”更像是由哪个“因”所导致的。特别地，

![截屏2020-02-29下午8.46.06](https://img.wush.cc/16311014457314.png?imageView2/0/format/webp/q/80)

表示最大后验概率（maximum a posteriori, MAP）估计，而

![截屏2020-02-29下午8.46.33](https://img.wush.cc/16311014457342.png?imageView2/0/format/webp/q/80)

表示最大似然估计（maximum likelihood estimation, MLE）。

![截屏2020-02-29下午8.48.29](https://img.wush.cc/16311014456752.png?imageView2/0/format/webp/q/80)

### 交叉验证t检验

**式** **(2.31)** **的解释**

第一，该式就是对式(2.28)到式(2.28)的一个具体应用。

第二，注意 2.4.2 节标题下第二行“其中 ![截屏2020-02-29下午8.49.50](https://img.wush.cc/16311014456773.png?imageView2/0/format/webp/q/80) 是在相同的第i折训练/测试集上得到的结果”，这一点在 2.2 节注解“交叉验证法的 Matlab 实现”中也特别强调了：“特别要注意的是，在对比不同算法的性能时，不同算法要使用相同的数据集划分，在留出法中也应该保证这一点”。

### McNemar检验

**列联表** **(contingency table)** **的解释**

第 41 页表 2.4 称为“列联表”，看论文时偶尔能遇到该术语（第 187 页第 2 行还会出现）。列联表是统计学中双变量相关分析时常见的概念。例如，现在要调查某医院各职业（医生、护士、行政人员）和性别（男、女）的相关关系，则需要先列出列联表：

![截屏2020-02-29下午8.51.30](https://img.wush.cc/16311014456793.png?imageView2/0/format/webp/q/80)

其中男医生人数为a ，男护士人数为b ，男行政人员人数为c ，女医生人数为 d，女护士人数为e ，女行政人员人数为f ，该医院总人数（或参与调查的人数）为a+b+c+d+e+f 。有了列联表，就可以计算一些统计量，如克莱姆相关系数。

列联表又称交叉资料表、交互分类表等，可参见百度百科词条列联表。

### Friedman检验与Nemenyi后续检验

很多时候基于诸如交叉验证 t 检验，在每个数据集上依次比较自己的算法与对比算法，发现自己的算法与对比算法在统计意义上并无显著差别，但是自己又会感觉自己的算法就是要好于对比算法，因为在每个数据集上的指标都略好于对比算法（虽然达不到统计意义上优于对方）。Friedman 检验基于算法排序比较各算法的性能，一定程度上可以克服这种情况。Friedman 检验的假设是在所有数据集上“所有算法性能相同”，若假设被拒绝则说明算法的性能显著不同，此时就需要使用 Nemenyi 后续检验进一步比较哪些算法之间性能有显著不同。Nemenyi 后续检验结果一般会画成图 2.8 的形式，可以直观地看出各算法性能之间的关系。

检验流程如下：

![截屏2020-02-29下午8.53.08](https://img.wush.cc/16311014457370.png?imageView2/0/format/webp/q/80)

其实算法两两之间也可以基于算法排序进行统计检验，这就是威尔科克森符号秩检验(Wilcoxon Signed-Ranks Test)，详见[Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. _Journal of Machine Learning Research_, 7(1), 1-30.]的第 3.1.3 节。

实际上，paired t-test 等直接基于指标数值的检验称为 parametric tests，而 Friedman 检验以及 Wilcoxon 检验等基于排序的检验称为 non-parametric tests。注意到书中介绍的 Nemenyi 后续检验针对的是把所有算法两两对比(when all classifiers are compared to each other)，而在实际中我们需要的是将自己提出的算法与对比算法比较即可(when all classifiers are compared with a control classifier)，这时就要使用 Bonferroni-Dunn检验而不是 Nemenyi 检验了，具体流程详见文章[Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. _Journal of Machine Learning Research_, 7(1), 1-30.]的第 3.2.2节，其实就是将式(2.36)中的 改变一下，即由论文中的 Table 5(a)换为 Table 5(b)。

## 偏差与方差

不考虑噪声，偏差很大可以认为是由模型欠拟合引起的，方差很大可以认为是由模型过拟合引起的。

### 数据集中的标记与真实标记

边注中已经提到，有可能出现噪声使得$y_D \not= y$。例如由众包（crowdsourcing）得到的标记就有可能是错的，详见作者的综述文章[Zhou, Z. H. . (2018). A brief introduction to weakly supervised learning. National Science Review, 5(01), 48-57.]的最后一部分内容“Inaccurate Supervision”。

### 式2.37到2.42推导

首先，梳理一下书中的符号：对测试样本x，令$y_D$为x在数据集中的标记，$y$为 x的真

实标记，$f(x;D)$为训练集D上学得模型f在x上的预测输出。

针对式(2.37)，可简单理解如下：

![截屏2020-02-29下午8.59.14](https://img.wush.cc/16311014456812.png?imageView2/0/format/webp/q/80)

针对式(2.38) ，可简单理解如下：

![截屏2020-02-29下午8.59.37](https://img.wush.cc/16311014456831.png?imageView2/0/format/webp/q/80)

针对式(2.39) ，可简单理解如下：

![截屏2020-02-29下午8.59.59](https://img.wush.cc/16311014456851.png?imageView2/0/format/webp/q/80)

有了以上基础，接下来推导最复杂的式(2.41)：

**第 1 个等号**：这是期望泛化误差的定义式，类似地，可直观地写为

![截屏2020-02-29下午9.00.33](https://img.wush.cc/16311014456868.png?imageView2/0/format/webp/q/80)

我们只能使用数据集中的标记$y_D$评估学习器泛化性能，因为真实标记y未知；

**2** **个等号**：常用的配项技巧，减去$\overline{f}(x)$再加上$\overline{f}(x)$，相当于没作任何变化；

**3** **个等号**：![截屏2020-02-29下午9.02.41](https://img.wush.cc/16311014456887.png?imageView2/0/format/webp/q/80)

![截屏2020-02-29下午9.03.06](https://img.wush.cc/16311014456906.png?imageView2/0/format/webp/q/80)

**4** **个等号**只须证明第 3 个等号最后一项等于 0；首先

![截屏2020-02-29下午9.03.29](https://img.wush.cc/16311014456928.png?imageView2/0/format/webp/q/80)

这就是使用了数学期望的性质，对于第一项：

![截屏2020-02-29下午9.03.57](https://img.wush.cc/16311014456948.png?imageView2/0/format/webp/q/80)

其中第 1 个等号就是乘开，把式子的扩号去掉；第 2 个等号是因为期望$\overline{f}(x)$为常量（式(2.37)），

因此$E_D[.]$运算不起作用；第 3 个等号使用了$\overline{f}(x)$的定义（式(2.37)）；对于第 2 项:

![截屏2020-02-29下午9.05.20](https://img.wush.cc/16311014456969.png?imageView2/0/format/webp/q/80)

其中第 1 个等号就是乘开，把式子的扩号去掉；第 2 个等号根据

![截屏2020-02-29下午9.05.51](https://img.wush.cc/16311014456992.png?imageView2/0/format/webp/q/80)

第 3 个等号中第 1 项

![截屏2020-02-29下午9.06.26](https://img.wush.cc/16311014457022.png?imageView2/0/format/webp/q/80)

根据数学期望的性质“当随机变量X和Y相互独立时，$E[XY]=E[][，而此处正如边注中所说“考虑到噪声不依赖于f ”，即$f(x;D)$与$y_D$相互独立（$y_D$等于y加上噪声，y为常量）；第 3 个等号中第 2项是因为期望$\overline{f}(x)$为常量；第 4 个等号使用了$\overline{f}(x)$的定义（式(2.37)）；

综上所述：式(2.41)中第 3 个等号最后一项等于 0，代入即得第 4 个等号；

**第** **5** **个等号**类似于第 2 个等号，对第 2 项使用了配项技巧；

**第** **6** **个等号**类似于第 3 个等号，对第 2 项使用了性质$E[X+Y]=E[]E[[[

**第** **7** **个等号**只须证明第 6 个等号最后一项等于 0；由于$y\overline{f}(x)$ 均为常量，因此

![截屏2020-02-29下午9.10.20](https://img.wush.cc/16311014457049.png?imageView2/0/format/webp/q/80)

上式第 3 个等号使用了等式

![截屏2020-02-29下午9.10.52](https://img.wush.cc/16311014457076.png?imageView2/0/format/webp/q/80)

这是由于在式(2.40)下方提到：为便于讨论，假定噪声期望为零，即![截屏2020-02-29下午9.11.23](https://img.wush.cc/16311014457104.png),其中y为x的真实标记，是一个常量；因此有![截屏2020-02-29下午9.12.12](https://img.wush.cc/16311014457130.png?imageView2/0/format/webp/q/80)。到此为止，式(2.41)推导结束。

针对式(2.42)，就是将式(2.38)、式(2.39)和式(2.40)代入到式(2.41)最后的表达式即可。

### 图2.9的解释

2.9 中横坐标表示的训练程度可以理解为模型的复杂程度。模型越简单（比如直线）其表达能力越弱，反之模型越复杂（比如 n 阶多项式）其表达能力越强。类似于图 2.9 的关系还有下图所示的训练误差(i.e., in-sample error)、模型复杂度(i.e., model complexity)和泛化误差(i.e., out-of-sample error)三者随着 VC 维（参见 12.4 节）的变化趋势关系

![截屏2020-02-29下午8.56.13](https://img.wush.cc/16311014457157.png?imageView2/0/format/webp/q/80)

实际上，上图与图 2.9 表示的问题本来也就是一回事……
